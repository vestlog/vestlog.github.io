---
categories:
  - 'Blog'
tags:
  - '네이버'
  - '파이썬'
  - '네이버'
  - 'CSV'
last_modified_at: '2025-05-30T16:04:04+09:00'
title: '[2025-08-13] - Post'
excerpt: ''
layout: 'post'
lang: 'ko'
permalink: '/posts/2020-12-03-python_웹크롤링을_통한_종가_정보_가져오기/'
alt_en: '/en/posts/2020-12-03-python_웹크롤링을_통한_종가_정보_가져오기/'
---

<div class="lang-panel lang-ko" lang="ko">
## [Python] 웹크롤링을 통한 종가 정보 가져오기

코딩정보/Python

2020-12-03 08:40:34

* * *

안녕하세요

코딩연습생입니다~ 이번 포스팅은 파이썬 언어를 통해 웹크롤링 기술을 사용하여

네이버 주가정보를 엑셀로 가져오는 기능을 구현해 볼려고 합니다

이 웹크롤링은 사용 부분이 참 많은 부분입니다 알아두면 어디든 활용할 부분이 참 많을거 같습니다

일단 크롤링이 무엇인지? 어떤 구조로 활용되는것인지 간단히 알아보고 넘어가도록 하겠습니다

크롤링(Cwawling) 이란?

\- Web상에 존재하는 Contents를 수집하는 작업

1) HTML 페이지를 가져와서, HTML/CSS 등을 파싱하고, 필요한 데이터만 추출하는 기법

2) Open API(Rest API)를 제공하는 서비스에 Open API를 호출해서, 받은 데이터 중 필요한 데이터만 추출하는 기법

3) Selenium등 브라우저를 프로그래밍으로 조작해서, 필요한 데이터만 추출하는 기법

구글에서 이런식으로 정의되어 있는거 같습니다ㅎ

일단 크롤링을 사용하기 위한 몇가지 라이브러리를 선언 합니다

1) BeautifulSoup

2) pandas

3) csv

4) re

5) requests

라이브러리 설치는 간단히 pip 명령어를 사용하여 설치 해주시면 됩니다

이제 네이버 주식정보를 크롤링하기 위한 함수를 구현해 보겠습니다

아래 함수 소스코드를 참고해주세요

여기서 outdirc변수에 불러온 주가정보를 저장할 경로를 지정하는 부분입니다

해당 주가코드에서 조회된 모든 내용을 크롤링 합니다

    
    
    def ReadStock(code):
        try:
            outdirc = settings.APP_CONFIG['HOME'] + settings.CRAWL_CONFIG['ticker_output_csv_path']
    
            url = f"http://finance.naver.com/item/sise_day.nhn?code={code}"
    
            with urlopen(url) as u:
                html = BeautifulSoup(u, "lxml")
                last = html.find("td", class_="pgRR")
                s = str(last.a["href"]).split('=')
                lastpage = s[-1]
    
            df = pd.DataFrame()
            #pages = 10  # int(lastpage)
            pages = int(lastpage)
            print("\n")
            for page in range(1, pages + 1):
                pg_url = f'{url}&page={page}'
                pValue = pd.read_html(pg_url, header=0)[0]
                df = df.append(pValue)
                print(f'downloading.. {page:04d}/{pages:04d} : ({code})', end="\r")
            print("\n")
            df = df.rename(columns={'날짜': 'date',
                                    '종가': 'close',
                                    '시가': 'open',
                                    '고가': 'high',
                                    '저가': 'low',
                                    '거래량': 'volume'})
            df = df.dropna() #결측값 행 제거
            df[['close', 'open', 'high', 'low', 'volume']] \
                = df[['close','open', 'high', 'low', 'volume']].astype(int)
    
        except Exception as e:
            print('Exception :', str(e))
            return None
        return df

크롤링이 발생되어야 할 구문에 다음과 같이 적용하여 실행 하면 됩니다

    
    
    stockValue = naver.ReadStock(stock_code)

stock_code는 상장코드로 기억의 6자리 코드를 넣어 주시면 됩니다

이렇게 해서 실행하면 제가 설정한 경로 안에 주식코드 6자리 형태의 csv 파일이 생성이 됩니다

![](/assets/images/python_웹크롤링을_통한_종가_정보_가져오기/img.png)

파일을 열어보면

![](/assets/images/python_웹크롤링을_통한_종가_정보_가져오기/img_1.png)

이렇게 파이썬 크롤링을 통한 종가 정보를 가져올 수 있습니다

  

#네이버 증권 #파이썬 크롤링 #네이버 증권 주식정보 가져오기 #CSV 파일 내보내기


</div>
<div class="lang-panel lang-en" lang="en">
(*No embedded English lines found. Add translations later.*)

</div>
